base_image:
  image:  vllm/vllm-openai:latest
model_name: google/gemma-3-27b-it
training_checkpoints:
  download_folder: /tmp/training_checkpoints
  checkpoints:
  - id: checkpoint-1
    name: checkpoint-1
docker_server:
  start_command: sh -c "HF_TOKEN=$(cat /secrets/hf_access_token) vllm serve google/gemma-3-27b-it --port 8000 --tensor-parallel-size 4 --enable-lora --max-lora-rank 16 --dtype bfloat16 --lora-modules checkpoint-1=/tmp/training_checkpoints/checkpoint-1"
  readiness_endpoint: /health
  liveness_endpoint: /health
  predict_endpoint: /v1/chat/completions
  server_port: 8000
resources:
  accelerator: H100:4
  use_gpu: true
runtime:
  predict_concurrency : 256
secrets:
  hf_access_token: set token in baseten workspace
environment_variables:
  VLLM_LOGGING_LEVEL: WARNING
  VLLM_USE_V1: 0
  HF_HUB_ENABLE_HF_TRANSFER: 1