FROM ghcr.io/huggingface/text-generation-inference:0.9.4
EXPOSE 8080

RUN apt-get update && DEBIAN_FRONTEND=noninteractive apt-get install -y --no-install-recommends \
        nginx supervisor curl && \
        rm -rf /var/lib/apt/lists/*

# TODO: add support for private models
{% if hf_access_token %}
ENV HUGGING_FACE_HUB_TOKEN {{hf_access_token}}
{% endif %}


{%- if hf_cache %}
COPY ./data /app/data
COPY ./cache_requirements.txt /app/cache_requirements.txt
RUN pip install -r /app/cache_requirements.txt --no-cache-dir && rm -rf /root/.cache/pip
COPY ./cache_warmer.py /cache_warmer.py
        {% for repo, hf_dir in models.items() %}
                {% for file in hf_dir.files %}
RUN python3 /cache_warmer.py {{file}} {{repo}} {% if hf_dir.revision != None %}{{hf_dir.revision}}{% endif %}
                {% endfor %}
        {% endfor %}
{%- endif %}

COPY ./proxy.conf /etc/nginx/conf.d/proxy.conf

RUN mkdir -p /var/log/supervisor
COPY supervisord.conf /etc/supervisor/conf.d/supervisord.conf

ENV SERVER_START_CMD /usr/bin/supervisord
ENTRYPOINT ["/usr/bin/supervisord"]
