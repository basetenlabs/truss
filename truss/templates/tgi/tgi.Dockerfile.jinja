FROM ghcr.io/huggingface/text-generation-inference:0.9.4
EXPOSE 8080

RUN apt-get update && DEBIAN_FRONTEND=noninteractive apt-get install -y --no-install-recommends \
        nginx supervisor curl && \
        rm -rf /var/lib/apt/lists/*

# TODO: add support for private models
{% if hf_access_token %}
ENV HUGGING_FACE_HUB_TOKEN {{hf_access_token}}
{% endif %}


{%- if hf_cache %}
RUN mkdir /app
        {%- if data_dir_exists %}
COPY ./data /app/data
        {%- endif %}
RUN curl -s https://baseten-public.s3.us-west-2.amazonaws.com/bin/b10cp-5fe8dc7da-linux-amd64 -o /app/b10cp; chmod +x /app/b10cp
ENV B10CP_PATH_TRUSS /app/b10cp
COPY ./cache_requirements.txt /app/cache_requirements.txt
RUN pip install -r /app/cache_requirements.txt --no-cache-dir && rm -rf /root/.cache/pip
COPY ./cache_warmer.py /cache_warmer.py
        {% for repo, hf_dir in models.items() %}
                {% for file in hf_dir.files %}
RUN python3 /cache_warmer.py {{file}} {{repo}} {% if hf_dir.revision != None %}{{hf_dir.revision}}{% endif %}
                {% endfor %}
        {% endfor %}
{%- endif %}

COPY ./proxy.conf /etc/nginx/conf.d/proxy.conf

RUN mkdir -p /var/log/supervisor
COPY supervisord.conf /etc/supervisor/conf.d/supervisord.conf

ENV SERVER_START_CMD /usr/bin/supervisord
ENTRYPOINT ["/usr/bin/supervisord"]
