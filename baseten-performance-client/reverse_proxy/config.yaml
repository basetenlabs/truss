# Baseten for `truss push --publish`
# For deployment on Baseten.
model_metadata:
  example_model_input: { "model": "Some model", "input": ["Dummy"] }
base_image:
  image: baseten/performance-proxy:0.0.2
docker_server:
  start_command: sh -c "echo 'starting' && sleep 1 && /usr/local/bin/baseten-performance-proxy --port 8081 --upstream-api-key /secrets/upstream_api_key --target-url ${UPSTREAM_URL} --tokenizer BAAI/bge-small-en-v1.5 /app/tokenizers/bge-small-en-v1.5/tokenizer.json --tokenizer voyageai/voyage-4-nano /app/tokenizers/voyage-4-nano/tokenizer.json --http-version 2 --max-chars-per-request 10000 --timeout-s 300 --batch-size 16 --max-concurrent-requests 64"
  readiness_endpoint: /health_internal
  liveness_endpoint: /health_internal
  predict_endpoint: /v1/embeddings
  server_port: 8081
build_commands:
  # Download tokenizers for multiple models
  - apt-get update && apt-get install -y wget && rm -rf /var/lib/apt/lists/*
  - sh -c "mkdir -p /app/tokenizers/bge-small-en-v1.5 && cd /app/tokenizers/bge-small-en-v1.5 && wget https://huggingface.co/BAAI/bge-small-en-v1.5/resolve/main/tokenizer.json -O tokenizer.json"
  - sh -c "mkdir -p /app/tokenizers/voyage-4-nano && cd /app/tokenizers/voyage-4-nano && wget https://huggingface.co/voyageai/voyage-4-nano/resolve/main/tokenizer.json -O tokenizer.json"
resources:
  use_gpu: true
  accelerator: L4
model_name: baseten-performance-proxy
environment_variables:
  # headers get stipped inside baseten. Therfore, you need to configure a UPSTREAM_URL and default auth header.
  UPSTREAM_URL: https://model-xxxxx.api.baseten.co/environments/production/sync
  PERFORMANCE_CLIENT_LOG_LEVEL: info
  NO_PROXY: "localhost,.baseten.co"
secrets:
  upstream_api_key: null # name saved in baseten-ui
