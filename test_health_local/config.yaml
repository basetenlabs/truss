base_image:
  image: vllm/vllm-openai:v0.6.1
docker_server:
  start_command: vllm serve meta-llama/Meta-Llama-3.1-8B-Instruct --port 8000 --max-model-len 1024
  readiness_endpoint: /health
  liveness_endpoint: /health
  predict_endpoint: /v1/chat/completions
  server_port: 8000
resources:
  accelerator: A10G
requirements:
  - boto3
model_name: docker server llama 8B tp 1
environment_variables:
  VLLM_LOGGING_LEVEL: DEBUG
runtime:
  predict_concurrency: 512
