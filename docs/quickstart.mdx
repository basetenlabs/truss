---
title: 'Quickstart'
description: 'Create, deploy, and invoke an ML model server in less than 5 minutes'
---

In this quickstart guide, we'll package and deploy a [text classification pipeline model](https://huggingface.co/docs/transformers/main_classes/pipelines) from the open-source `transformers` package.

If you want to go step-by-step through these concepts and more, check out the [learn model deployment tab](/learn/intro) for a detailed introduction to model deployment and Truss.

## Install Truss

Install the latest version of Truss with:

```sh
pip install --upgrade truss
```

## Create a Truss

To get started, create a Truss with the following terminal command:

```sh
truss init text-classification
```

Select the default `TrussServer` option. Then, navigate to the newly created directory:

```sh
cd text-classification
```

## Implement the `Model` class

The `Model` class is the heart of a Truss. In it, you write similar code to what you'd write to invoke a model locally in a Python notebook or script.

<Frame caption="Placeholder image, will get a better one made">
  <img src="/images/notebook-to-model.png" />
</Frame>

The model serving code goes in `./text-classification/model/model.py` in your newly created Truss. There are two functions to implement:

* `load()` runs once when the model is spun up and is responsible for initializing `self._model`
* `predict()` runs each time the model is invoked and handles the inference. It can use any JSON-serializable type as input and output.

Here's the complete `model/model.py` for the text classification model:

```python model/model.py
from typing import List
from transformers import pipeline


class Model:
    def __init__(self, **kwargs) -> None:
        self._model = None

    def load(self):
        self._model = pipeline("text-classification")

    def predict(self, model_input: str) -> List:
        return self._model(model_input)
```

## Add model dependencies

The pipeline model relies on Transformers and PyTorch. These dependencies must be specified in the Truss config.

In `./text-classification/config.yaml`, find the line `requirements`. Replace the empty list with:

```yaml config.yaml
requirements:
  - torch==2.0.1
  - transformers==4.30.0
```

No other configuration needs to be changed.

## Deploy the Truss

Truss is maintained by [Baseten](https://baseten.co), which provides infrastructure for running ML models in production. We'll use Baseten as the remote host for your model.

### Get an API key

To set up the Baseten remote, you'll need a [Baseten API key](https://app.baseten.co/settings/account/api_keys). If you don't have a Baseten account, no worries, just [sign up for an account](https://app.baseten.co/signup/) and you'll be issued plenty of free credits to get you started.

### Run `truss push`

Deploying a model with Truss uses the interactive `truss push` command. The first time you run the command, it will walk you through setting up and a remote host to run your model.

With your Baseten API key ready to paste when prompted, you can deploy your model:

```sh
truss push
```

You can monitor your model deployment on [your model dashboard on Baseten](https://app.baseten.co/models/).

## Invoke the model

After the model has finished deploying, you can invoke it from the terminal.

**Invocation**

```sh
truss predict -d '"I am happy!"'
```

**Response**

```json
[
  {
    "label": "POSITIVE",
    "score": 0.999873161315918
  }
]
```
