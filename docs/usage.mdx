---
title: User guide
description: "Get up and running quickly with Truss' developer workflow"
---

We built Truss because we were frustrated with the long feedback loops in ML model deployment. When you have to wait for your server to rebuild every time you make a change, iteration is painful.

Meanwhile, web developers have enjoyed live reload workflows for years, where changes are patched onto a running server and available almost instantly.

With Truss, you get the same live reload workflow for serving ML models.

<Frame caption="Truss' live reload model serving workflow">
<img src="/images/user-workflow.png" />
</Frame>

## Create a Truss

Lorem

```sh
truss init
```

[More information on truss init](/reference/cli/init)

## Spin up model server

Lorem

```sh
truss push
```

[More information on truss init](/reference/cli/push)

## Work with live reload

Lorem

```sh
truss watch
```

<Tip>
Run the `truss watch` command in a new terminal tab in the same working directory, as you'll need to leave it running while you work.
</Tip>

[More information on truss watch](/reference/cli/watch)

### Watch for changes

When you make a change with `truss watch` running, it will automatically attempt to patch that change onto the model server. Most changes to `model.py` and `config.yaml` can be patched.

<Warning>
The following changes should not be made in a live reload workflow:

* Updates to `resources` in `config.yaml`, which must be set before the first `truss push`
* Changes to the `model_name` in `config.yaml`. Changing the model name requires a new `truss push` to create a new model server.
</Warning>

### Test the model

Lorem

```sh
truss predict
```

[More information on truss predict](/reference/cli/predict)

## Publish your model

Once you're happy with your model, stop `truss watch` and run:

```sh
truss push --publish
```

This will re-build your model server on production infrastructure.

## Use model in production

To invoke the published model, run:

```sh
truss predict --published
```

With [Baseten](https://baseten.co) as your [remote host](/remotes/baseten), your model is served behind [autoscaling infrastructure](https://docs.baseten.co/managing-models/resources) and is [available via an API endpoint](https://docs.baseten.co/building-with-models/invoke).
