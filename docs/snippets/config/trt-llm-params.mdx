### `trt_llm`

Configuration for TRT-LLM accelerated model services
#### `build`

TRT-LLM engine build configuration
##### `base_model`

The base model architecture of your model checkpoint. Supported architectures include:

* `llama`
* `mistral`
* `deepseek`
* `whisper`

##### `max_input_len`

Maximum input length in tokens to limit prompts to

##### `max_output_len`

Maximum output length in tokens to limit prompts to

##### `max_batch_size`

Maximum number of sequences to pass through the engine consecutively

##### `max_beam_width`

Maximum number of candidate sequences with which to conduct beam search

##### `max_prompt_embedding_table_size`
(default: 0)

Maximum prompt embedding table size

##### `checkpoint_repository`

Specification of the model checkpoint to be leveraged for engine building

###### `checkpoint_repository.source`

Source for where the checkpoint is stored. Supported sources include:
* `HF` (HuggingFace)
* `GCS` (Google Cloud Storage)
* `REMOTE_URL`

###### `checkpoint_repository.repo`

Checkpoint repository name, bucket, or remote url

##### `gather_all_token_logits`
(default: False)

Whether or not to gather all token logits

##### `strongly_typed`
(default: False)

Whether to build the engine using strong typing, can speed up build time for some formats

##### `quantization_type`
(default: no_quant)

Quantization format with which to build the engine. Supported formats include:
* `no_quant`
* `weights_int8`
* `weights_kv_int8`
* `weights_int4`
* `weights_kv_int4`
* `smooth_quant`
* `fp8`
* `fp8_kv`

Read more about the different quantization techniques [here](https://github.com/NVIDIA/TensorRT-LLM/blob/main/docs/source/blogs/quantization-in-TRT-LLM.md)

##### `tensor_parallel_count`
(default: 1)

Tensor parallelism count

##### `pipeline_parallel_count`
(default: 1)

Pipeline parallelism count

##### `plugin_configuration`

Config for inserting plugin nodes into network graph definition for execution of user-defined kernels

###### `plugin_configuration.multi_block_mode`
(default: False)

Distribute masked MHA kernel work across multiple CUDA thread blocks in scenarios with low GPU occupancy. Read more about when to enable this plugin [here](https://github.com/NVIDIA/TensorRT-LLM/blob/main/docs/source/advanced/gpt-attention.md#generation-phase)

###### `plugin_configuration.paged_kv_cache`
(default: True)

Decompose KV cahce into page blocks. Read more about what this does [here](https://github.com/NVIDIA/TensorRT-LLM/blob/main/docs/source/advanced/gpt-attention.md#paged-kv-cache).

###### `plugin_configuration.gemm_plugin`
(default: float16)

Utilize NVIDIA cuBLASLt for GEMM ops. Read more about when to enable this [here](https://github.com/NVIDIA/TensorRT-LLM/blob/main/docs/source/performance/perf-best-practices.md#gemm-plugin)

##### `use_fused_mlp`
(default: False)

Enables GEMM horizontal fusion in gated MLP layer, potentially improving performance

##### `kv_cache_free_gpu_mem_fraction`
(default: 0.9)

Used to control the fraction of free gpu memory allocated for the KV cache

##### `num_builder_gpus`

Number of GPUs to be used at build time
