---
title: "Step 7: Implement model inference"
description: "Implement invocation for our new ML model"
---

To finish this tutorial, we'll implement model inference and invoke our finished model.

### Run model inference

In Truss, the `Model.predict()` function runs every time the model server endpoint is called.

We'll call the model in `predict()` and return the results:

<Tabs>
<Tab title="Code">
```python model/model.py
def predict(self, model_input: Any) -> Any:
    return self._model(model_input)
```
</Tab>
<Tab title="Diff">
```diff model/model.py
def predict(self, model_input: Any) -> Any:
-    return model_input
+    return self._model(model_input)
```
</Tab>
</Tabs>

## Invoke your finished model

After `truss watch` shows that the server is updated, you're ready to go! If you haven't been using `truss watch`, you can update your model manually by running `truss push` again instead.

Once the model server is ready, you can invoke your model the same was as before.

**Invocation**

```sh
truss predict -d '"I am happy!"'
```

**Response**

```json
[
  {
    "label": "POSITIVE",
    "score": 0.999873161315918
  }
]
```

<RequestExample>

```python model/model.py  â—
from typing import Any
from transformers import pipeline


class Model:
    def __init__(self, **kwargs) -> None:
        self._data_dir = kwargs["data_dir"]
        self._config = kwargs["config"]
        self._secrets = kwargs["secrets"]
        self._model = None

    def load(self):
        self._model = pipeline("text-classification")

    def predict(self, model_input: Any) -> Any:
        return self._model(model_input)
```

<Snippet file="base-config.mdx" />

</RequestExample>
