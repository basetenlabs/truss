---
title: "Step 1: Echo server"
description: "Description"
---

In this step, we'll implement a simple echo server.

A model server's `predict()` function matches up with its `predict` endpoint: the way you actually run model inference. But we can start seeing how that works without bothering with a model yet.

Open `model/model.py` and update `predict()` to echo the `model_input`:

<Tabs>
<Tab title="Code">
```python model/model.py
def predict(self, model_input: Any) -> Any:
    return model_input
```
</Tab>
<Tab title="Diff">
```diff model/model.py
def predict(self, model_input: Any) -> Any:
-    model_output = {}
-    # Invoke model on model_input and calculate predictions here.
-    model_output["predictions"] = []
-    return model_output
+    return model_input
```
</Tab>
</Tabs>

<RequestExample>

```python model/model.py  ●
from typing import Any


class Model:
    def __init__(self, **kwargs) -> None:
        self._data_dir = kwargs["data_dir"]
        self._config = kwargs["config"]
        self._secrets = kwargs["secrets"]
        self._model = None

    def load(self):
        # Load model here and assign to self._model.
        pass

    def predict(self, model_input: Any) -> Any:
        return model_input
```

<Snippet file="base-config.mdx" />

</RequestExample>
