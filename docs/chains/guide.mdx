---
title: Guide
description: "Tips, Tricks and Best Practices"
---

{/* This page is currently a collection of content that has not found a good
other place or is WIP. Content might be split out to other pages for better
organization. */}

<Warning>*Chains* is a beta feature and subject to breaking changes.</Warning>

import TOC from '/snippets/chains/TOC.mdx';

<TOC />

# Development Experience

Truss Chains is designed to run in a distributed, replicated environment - as
[baseten deployments](/chains/getting-started#remote-deployment-to-baseten) -
but to enable a rapid development experience, Truss Chains additionally has
a [local debugging mode](/chains/guide#local-debugging-with-mock) which allows
you to iterate more quickly.

Even though Chains might sometimes *feel* like working with "normal, local"
python code, there are some constraints and rules that must be followed to
enable successful distributed, remote execution.

The Chains framework runs some validations <Tooltip tip="This means at python
module initialization, i.e. before executing code.">for each defined Chainlet
subclasses</Tooltip> and if problems are found, error messages with suggestions
are shown. So by frequently running your file, you'll be notified early of
potential issues (i.e. just run `python <YOUR_FILE>`, even if it has no
`__main__` section).

### Local Debugging

Truss Chains supports a
[run local mode](chains/deps-and-resources#running-it-locally) intended for
debugging and testing of your Chainlets.
To demonstrate it, we extend the example from
[getting started](/chains/getting-started) and append the following
to the `hello.py` file:

```python
if __name__ == "__main__":
    with chains.run_local():
        hello_world_chain = HelloWorld()
        result = hello_world_chain.run_remote(max_value=5)

    print(result)
    # Hello World! Hello World! Hello World!
```

If you run this local instances of the chainlets are created and you can test
some functionality of your chain:

```bash
python hello.py
# Hello World! Hello World! Hello World! Hello World!
```

<Warning>
Note `run_local` uses whatever compute resources and dependencies available in
your dev environment. If your Chainlet requires a specific GPU or Python
dependencies that are not available, it will not work. For such cases refer
to [mocking parts](chains/deps-and-resources#running-it-locally) of a Chain.
Also note that `run_local` does not have 1:1 the same behavior as the deployed
chain and some problems might only surface after deploying.
</Warning>


# LLM Chain with special requirements

In this section, we add a Chainlet that requires a GPU and other extra
resources and show how resources can be different for different Chainlets.

For this Chain, we define one chainlet that takes in a list of words, and for
each word calls an LLM that creates a poem about that word.

Architecture:

<img src="/images/mistral-diagram.png" />


<Note>
To access the Mistral model, used in this guide, you need to agree to the terms
on [the model page](https://huggingface.co/mistralai/Mistral-7B-Instruct-v0.2), 
[obtain]((https://huggingface.co/settings/tokens) a Hugging Face access token and add it as a
[secret on baseten](https://app.baseten.co/settings/secrets),
using the key `"hf_access_token"`.
</Note>

### Configuring requirements

The main difference between this Chain and the one from [getting started](/chains/getting-started)
is that we add a model that requires a GPU and
large model weights - the Mistral 7B LLM.

The snippet below illustrates how to define such a Chainlet (copy it into a file called
`poems.py`.)

```python
import truss_chains as chains
from truss import truss_config

MISTRAL_HF_MODEL = "mistralai/Mistral-7B-Instruct-v0.2"
# This configures to cache model weights from the hunggingface repo
# in the docker image that is used for deploying the Chainlet.
MISTRAL_CACHE = truss_config.ModelRepo(
    repo_id=MISTRAL_HF_MODEL, allow_patterns=["*.json", "*.safetensors", ".model"]
)

class MistralLLM(chains.ChainletBase):
    # The RemoteConfig object defines the resources required for this chainlet.
    remote_config = chains.RemoteConfig(
        docker_image=chains.DockerImage(
            # The mistral model needs some extra python packages.
            pip_requirements=[
                "transformers==4.38.1",
                "torch==2.0.1",
                "sentencepiece",
                "accelerate",
            ]
        ),
        # The mistral model needs a GPU and more CPUs.
        compute=chains.Compute(cpu_count=2, gpu="A10G"),
        # Cache the model weights in the image and make the huggingface
        # access token secret available to the model.
        assets=chains.Assets(cached=[MISTRAL_CACHE], secret_keys=["hf_access_token"]),
    )

    def __init__(
        self,
        # Adding the `context` to the init arguments, allows us to access the
        # huggingface token.
        context: chains.DeploymentContext = chains.depends_context(),
    ) -> None:
        # Note the imports of the *specific* python requirements are pushed down to
        # here. This code will only be executed on the remotely deployed chainlet,
        # not in the local environment, so we don't need to install these packages
        # in the local dev environment.
        import torch
        import transformers

        self._model = transformers.AutoModelForCausalLM.from_pretrained(
            MISTRAL_HF_MODEL,
            torch_dtype=torch.float16,
            device_map="auto",
            use_auth_token=context.secrets["HF_ACCESS_TOKEN"],
        )

        self._tokenizer = transformers.AutoTokenizer.from_pretrained(
            MISTRAL_HF_MODEL,
            device_map="auto",
            torch_dtype=torch.float16,
            use_auth_token=context.secrets["HF_ACCESS_TOKEN"],
        )

        self._generate_args = {
            "max_new_tokens": 512,
            "temperature": 1.0,
            "top_p": 0.95,
            "top_k": 50,
            "repetition_penalty": 1.0,
            "no_repeat_ngram_size": 0,
            "use_cache": True,
            "do_sample": True,
            "eos_token_id": self._tokenizer.eos_token_id,
            "pad_token_id": self._tokenizer.pad_token_id,
        }

    def run_remote(self, data: str) -> str:
        import torch

        formatted_prompt = f"[INST] {data} [/INST]"
        input_ids = self._tokenizer(
            formatted_prompt, return_tensors="pt"
        ).input_ids.cuda()
        with torch.no_grad():
            output = self._model.generate(inputs=input_ids, **self._generate_args)
            result = self._tokenizer.decode(output[0])
        return result
```



### Chaining it all together

Now that we have an LLM, we can use it in a poem generator Chainlet. Add the
following code to `poems.py`:

```python
@chains.mark_entrypoint
class PoemGenerator(chains.ChainletBase):
    def __init__(self, mistral_llm: MistralLLM = chains.depends(MistralLLM)) -> None:
        self._mistral_llm = mistral_llm

    def run_remote(self, words: list[str]) -> list[str]:
        results = []
        for word in words:
            poem = self._mistral_llm.run_remote(f"Generate a poem about: {word}")
            results.append(poem)
        return results

```

Deploy the chain with:

```bash
truss chains deploy poems.py
```

And call it:
```bash
curl -X POST 'https://model-4w7g6pyw.api.baseten.co/development/predict' \
    -H "Authorization: Api-Key $BASETEN_API_KEY" \
    -d '{"words": ["bird", "plane", "superman"]}'
#[[
#"<s> [INST] Generate a poem about: bird [/INST] In the quiet hush of...</s>",
#"<s> [INST] Generate a poem about: plane [/INST] In the vast, boudl...</s>",
#"<s> [INST] Generate a poem about: superman [/INST] In the realm where...</s>"
#]]
```


### Local Debugging with Mock

Our Chain includes the `MistralLLM` Chainlet which requires specific Python dependencies
and a powerful GPU that you may not have access to in your local development environment.
The other Chainlet, `PoemGenerator`, contains simpler "coordination" logic and has less demanding requirements.

For quicker iteration, we encourage debugging and testing such "mixed" Chains locally.
In that case, you can mock the parts that are infeasible
to run locally and test all the other components - especially the
interplay and controlflow of multiple Chainlets in more complicated Chains.

To do this, define a fake Mistral model class whose `run`-method implements the
same [Protocol](https://typing.readthedocs.io/en/latest/spec/protocol.html)
as the original `MistralLLM` Chainlet, and produces some
test output. Then "inject" an instance of `FakeMistralLLM` into your Chain
like so:

```python
if __name__ == "__main__":
    class FakeMistralLLM:
        def run_remote(self, data: str) -> str:
            return "Fake mistral response"


    with chains.run_local():
        poem_generator = PoemGenerator(mistral_llm=FakeMistralLLM())
        result = poem_generator.run_remote(words=["bird", "plane", "superman"])
        print(result)
```

And run your Python file:

```bash
python poems.py
# ['Fake mistral response', 'Fake mistral response', 'Fake mistral response']
```

<Note>
The argument mistral_llm is of type Mistral LLM that can be substituted
by an instance of type FakeMistralLLM because FakeMistralLLM and MistralLLM have the same protocol.
This can be made explicit by using `Protocol` type annotation: 

```python
from typing import Protocol

class MistralP(Protocol):
    def run_remote(self, data: str) -> str:
        ...
```

and change the argument type in `PoemGenerator`:

```python
@chains.mark_entrypoint
class PoemGenerator(chains.ChainletBase):
    def __init__(self, mistral_llm: MistralP = chains.depends(MistralLLM)) -> None:
        self._mistral_llm = mistral_llm
```

</Note>
