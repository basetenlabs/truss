---
title: Guide
description: "Tips, Tricks and Best Practices"
---

{/* This page is currently a collection of content that has not found a good
other place or is WIP. Content might be split out to other pages for better
organization. */}

<Warning>*Chains* is a beta feature and subject to breaking changes.</Warning>

import TOC from '/snippets/chains/TOC.mdx';

<TOC />

# Development Experience

Truss Chains is designed to run in a distributed, replicated environment - as
[baseten deployments](/chains/getting-started#remote-deployment-to-baseten) -
but to enable a rapid development experience, Truss Chains additionally has
a [local debugging mode](/chains/guide#local-debugging-with-mock) which allows
you to iterate more quickly.

Even though Chains might sometimes *feel* like working with "normal, local"
python code, there are some constraints and rules that must be followed to
enable successful distributed, remote execution.

The Chains framework runs some validations <Tooltip tip="This means at python
module initialization, i.e. before executing code.">for each defined Chainlet
subclasses</Tooltip> and if problems are found, error messages with suggestions
are shown. So by frequently running your file, you'll be notified early of
potential issues (i.e. just run `python <YOUR_FILE>`, even if it has no
`__main__` section).

### Local Debugging

Truss Chains supports a
[run local mode](chains/deps-and-resources#running-it-locally) intended for
debugging and testing of your Chainlets.
To demonstrate it, we extend the example from
[getting started](/chains/getting-started) and append the following
to the `hello.py` file:

```python
if __name__ == "__main__":
    with chains.run_local():
        hello_world_chain = HelloWorld()
        result = hello_world_chain.run_remote(max_value=5)

    print(result)
    # Hello World! Hello World! Hello World!
```

If you run this local instances of the chainlets are created and you can test
some functionality of your chain:

```bash
python hello.py
# Hello World! Hello World! Hello World! Hello World!
```

<Warning>
Note `run_local` uses whatever compute resources and dependencies available in
your dev environment. If your Chainlet requires a specific GPU or Python
dependencies that are not available, it will not work. For such cases refer
to [mocking parts](chains/deps-and-resources#running-it-locally) of a Chain.
Also note that `run_local` does not have 1:1 the same behavior as the deployed
chain and some problems might only surface after deploying.
</Warning>


# LLM Chain with special requirements

In this section, we add a Chainlet that requires a GPU and other extra
resources and show how resources can be different for different Chainlets.

For this Chain, we define one chainlet that takes in a list of words, and for
each word calls an LLM that creates a poem about that word.

Architecture:

<img src="/images/mistral-diagram.png" />


<Note>
To access the Mistral model, used in this guide, you need to accept
the access policy on
[huggingface](https://huggingface.co/mistralai/Mistral-7B-Instruct-v0.2),
get a huggingface access token and add it as a
[secret on baseten](https://app.baseten.co/settings/secrets),
using the key `"hf_access_token"`.
</Note>

### Configuring requirements

The main difference between this Chain and the one from [getting started]
(/chains/getting-started) is that we add a heavy model that needs a GPU and
large model weights - the Mistral7B LLM.

Below code shows how to define such a Chainlet (copy it into a file called
`poems.py`.)

```python
import truss_chains as chains
from truss import truss_config

MISTRAL_HF_MODEL = "mistralai/Mistral-7B-Instruct-v0.2"
# This configures to cache model weights from the hunggingface repo
# in the docker image that is used for deploying the Chainlet.
MISTRAL_CACHE = truss_config.ModelRepo(
    repo_id=MISTRAL_HF_MODEL, allow_patterns=["*.json", "*.safetensors", ".model"]
)

class MistralLLM(chains.ChainletBase):
    # The RemoteConfig object defines the resources required for this chainlet.
    remote_config = chains.RemoteConfig(
        docker_image=chains.DockerImage(
            # The mistral model needs some extra python packages.
            pip_requirements=[
                "transformers==4.38.1",
                "torch==2.0.1",
                "sentencepiece",
                "accelerate",
            ]
        ),
        # The mistral model needs a GPU and more CPUs.
        compute=chains.Compute(cpu_count=2, gpu="A10G"),
        # Cache the model weights in the image and make the huggingface
        # access token secret available to the model.
        assets=chains.Assets(cached=[MISTRAL_CACHE], secret_keys=["hf_access_token"]),
    )

    def __init__(
        self,
        # Adding the `context` to the init arguments, allows us to access the
        # huggingface token.
        context: chains.DeploymentContext = chains.depends_context(),
    ) -> None:
        # Note the imports of the *specific* python requirements are pushed down to
        # here. This code will only be executed on the remotely deployed chainlet,
        # not in the local environment, so we don't need to install these packages
        # in the local dev environment.
        import torch
        import transformers

        self._model = transformers.AutoModelForCausalLM.from_pretrained(
            MISTRAL_HF_MODEL,
            torch_dtype=torch.float16,
            device_map="auto",
            use_auth_token=context.secrets["HF_ACCESS_TOKEN"],
        )

        self._tokenizer = transformers.AutoTokenizer.from_pretrained(
            MISTRAL_HF_MODEL,
            device_map="auto",
            torch_dtype=torch.float16,
            use_auth_token=context.secrets["HF_ACCESS_TOKEN"],
        )

        self._generate_args = {
            "max_new_tokens": 512,
            "temperature": 1.0,
            "top_p": 0.95,
            "top_k": 50,
            "repetition_penalty": 1.0,
            "no_repeat_ngram_size": 0,
            "use_cache": True,
            "do_sample": True,
            "eos_token_id": self._tokenizer.eos_token_id,
            "pad_token_id": self._tokenizer.pad_token_id,
        }

    def run_remote(self, data: str) -> str:
        import torch

        formatted_prompt = f"[INST] {data} [/INST]"
        input_ids = self._tokenizer(
            formatted_prompt, return_tensors="pt"
        ).input_ids.cuda()
        with torch.no_grad():
            output = self._model.generate(inputs=input_ids, **self._generate_args)
            result = self._tokenizer.decode(output[0])
        return result
```



### Chaining it all together

Now that we have an LLM, we can use it in a poem generator Chainlet. Add the
following code to `poems.py`:

```python
@chains.mark_entrypoint
class PoemGenerator(chains.ChainletBase):
    def __init__(self, mistral_llm: MistralLLM = chains.depends(MistralLLM)) -> None:
        self._mistral_llm = mistral_llm

    def run_remote(self, words: list[str]) -> list[str]:
        results = []
        for word in words:
            poem = self._mistral_llm.run_remote(f"Generate a poem about: {word}")
            results.append(poem)
        return results

```

Deploy the chain with:

```bash
truss chains deploy poems.py
```

And call it:
```bash
curl -X POST 'https://model-4w7g6pyw.api.baseten.co/development/predict' \
    -H "Authorization: Api-Key $BASETEN_API_KEY" \
    -d '{"words": ["bird", "plane", "superman"]}'
#[[
#"<s> [INST] Generate a poem about: bird [/INST] In the quiet hush of...</s>",
#"<s> [INST] Generate a poem about: plane [/INST] In the vast, boudl...</s>",
#"<s> [INST] Generate a poem about: superman [/INST] In the realm where...</s>"
#]]
```


### Local Debugging with Mock

Our Chain includes the `MistralLLM` Chainlet which requires heavy hardware and
python dependencies. These requirements are fulfilled in the remote baseten
deployment, but most likely not locally. The other Chainlet, `PoemGenerator`,
contains simpler "coordination" logic and has less demanding requirements.

We still encourage debugging and testing such "mixed" Chains locally (for a
quicker dev loop). In that case, you can mock the parts that are infeasible
to run locally and test all the other components - especially the
interplay and controlflow of multiple Chainlets in more complicated Chains.

To do this, define a fake Mistral model class whose `run`-method implements the
same *Protocol* as the original `MistralLLM` Chainlet, and produces some
test output. Then "inject" an instance of `FakeMistralLLM` into your Chain
like so:

```python
if __name__ == "__main__":
    class FakeMistralLLM:
        def run_remote(self, data: str) -> str:
            return "Fake mistral response"


    with chains.run_local():
        poem_generator = PoemGenerator(mistral_llm=FakeMistralLLM())
        result = poem_generator.run_remote(words=["bird", "plane", "superman"])
        print(result)
```

And run your Python file:

```bash
python poems.py
# ['Fake mistral response', 'Fake mistral response', 'Fake mistral response']
```

<Note>
This is strictly a type error: the argument `mistral_llm` of type
`MistralLLM` we pass an instance of `FakeMistralLLM` - which is not the
same.
It still works at runtime, because we constructed `FakeMistralLLM` to
have the same *procotol*. To also get this "clean" from a typing perspective,
we can actually use a `Protocol` in the type annotation:

```python
from typing import Protocol

class MistralP(Protocol):
    def run_remote(self, data: str) -> str:
        ...
```

and change the argument type in `PoemGenerator`:

```python
@chains.mark_entrypoint
class PoemGenerator(chains.ChainletBase):
    def __init__(self, mistral_llm: MistralP = chains.depends(MistralLLM)) -> None:
        self._mistral_llm = mistral_llm
```

</Note>
