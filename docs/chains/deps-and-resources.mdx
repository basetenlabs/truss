---
title: Dependencies and Resources
description: "Including a Chainlet Requiring a GPU"
---

<Warning>*Chains* is a beta feature and subject to breaking changes.</Warning>

# Table of Contents

* [Introduction](/chains/intro)
* [Getting Started](/chains/getting-started)
* [Chaining Chainlets](/chains/chaining-chainlets)
* [Dependencies & Resources](/chains/deps-and-resources)
* [Reference](/chains/full-reference)

In the previous guide, we went through how to combine two different Chainlets in a single Chain.
In this guide, we'll go through adding a Chainlet that requires a GPU, to demonstrate how resources can
be different for different Chainlets.

To demonstrate this, we build a Chain that takes in a list of words, and for each each word, returns
a poem inspired by that word.

Architecture:

<img src="/images/mistral-diagram.png" />


# Changing Compute Resources

The main difference between this Chain and the previous one is that for this Chain, we'll
need a GPU to run the LLM model. In this example, we'll use Mistral for our LLM.

Before we introduce the Mistral code, let's create a python file `poems.py` and start with the resources required for
this Chain.

```python

import truss_chains as chains
import pydantic
from truss import truss_config


class MistraLLMConfig(pydantic.BaseModel):
    hf_model_name: str

class MistralLLM(chains.ChainletBase[MistraLLMConfig]):
    # The RemoteConfig object defines the resources required for this chainlet
    remote_config = chains.RemoteConfig(
        # The DockerImage object defines properties of the docker image.
        # Here, we can define the pip and system dependencies required for the chainlet.
        docker_image=chains.DockerImage(
            pip_requirements=[
                "git+https://github.com/basetenlabs/truss.git"
                "transformers==4.38.1",
                "torch==2.0.1",
                "sentencepiece",
                "accelerate",
            ],
        ),
        # The Compute object defines the compute resources required for the chainlet.
        # Here, we specify that we require an A10G GPU
        compute=chains.Compute(cpu_count=2, gpu="A10G"),
    )

    def __init__(
        self,
        context: chains.DeploymentContext = chains.provide_context(),
    ) -> None:
        super().__init__(context)
        ...


    def run(self, data: str) -> str:
        ...

```

# The Mistral Code

Once we've defined the needed resources, we can now add the Mistral code to the Chainlet.

```python
import truss_chains as chains
from truss import truss_config
import pydantic

MISTRAL_HF_MODEL = "mistralai/Mistral-7B-Instruct-v0.2"

class MistraLLMConfig(pydantic.BaseModel):
    hf_model_name: str

class MistralLLM(chains.ChainletBase[MistraLLMConfig]):
    remote_config = chains.RemoteConfig(
        docker_image=chains.DockerImage(
            pip_requirements=[
                "git+https://github.com/basetenlabs/truss.git",
                "transformers==4.38.1",
                "torch==2.0.1",
                "sentencepiece",
                "accelerate",
            ],
        ),
        compute=chains.Compute(cpu_count=2, gpu="A10G"),
        # This is one of the main things that we've added. This shows off using
        # the Model Cache feature of Truss to cache the weights of the model at
        # build time.
        assets=chains.Assets(cached=[
            truss_config.ModelRepo(
                repo_id=MISTRAL_HF_MODEL, allow_patterns=["*.json", "*.safetensors", ".model"]
            )
        ]),
    )
    default_user_config = MistraLLMConfig(hf_model_name=MISTRAL_HF_MODEL)

    def __init__(
        self,
        context: chains.DeploymentContext[MistraLLMConfig] = chains.provide_context(),
    ) -> None:
        super().__init__(context)
        # `torch` and `transformers` are imported here, rather than the top-level, so that
        # the other Chainlets we define in this file don't crash when torch & transformers
        # are not available in their envs.
        import torch
        import transformers

        model_name = self.user_config.hf_model_name

        self._model = transformers.AutoModelForCausalLM.from_pretrained(
            model_name,
            torch_dtype=torch.float16,
            device_map="auto",
        )
        self._tokenizer = transformers.AutoTokenizer.from_pretrained(
            model_name,
            device_map="auto",
            torch_dtype=torch.float16,
        )

        self._generate_args = {
            "max_new_tokens": 512,
            "temperature": 1.0,
            "top_p": 0.95,
            "top_k": 50,
            "repetition_penalty": 1.0,
            "no_repeat_ngram_size": 0,
            "use_cache": True,
            "do_sample": True,
            "eos_token_id": self._tokenizer.eos_token_id,
            "pad_token_id": self._tokenizer.pad_token_id,
        }

    def run(self, prompt: str) -> str:
        import torch

        formatted_prompt = f"[INST] {data} [/INST]"
        input_ids = self._tokenizer(
            formatted_prompt, return_tensors="pt"
        ).input_ids.cuda()
        with torch.no_grad():
            output = self._model.generate(inputs=input_ids, **self._generate_args)
            result = self._tokenizer.decode(output[0])
        return result
```

# Chaining it all together

Once we have our LLM that can produce results, we can Chain it with our Chainlet that generates poems.

```python
import truss_chains as chains

class PoemGenerator(chains.ChainletBase):
    remote_config = chains.RemoteConfig(
        docker_image=chains.DockerImage(
            pip_requirements=["git+https://github.com/basetenlabs/truss]
        )
    )

    def __init__(
        self,
        context: chains.DeploymentContext = chains.provide_context(),
        mistral_llm: MistralLLM = chains.provide(MistralLLM),
    ) -> None:
        super().__init__(context)
        self._mistral_llm = mistral_llm

    def run(self, words: list[str]) -> list[str]:
        results = []
        for word in words:
            poem = self._mistral_llm.run(f"Generate a poem about: {word}")
            results.append(poem)
        return results
```

## Local Debugging

Or Chain includes the `MistralLLM` Chainlet which requires heavy hardware and python dependencies. These
 requirements are fulfilled in the remote baseten deployment, but most likely not locally. The other Chainlet,
 `PoemGenerator`, contains simpler "coordination" logic and has les demanding requirements.

We still encourage debugging and testing such "mixed" Chains locally (for a quicker dev loop). In that case,
you can mock the parts that are infeasible to run locally and test all the other components - especially the
interplay and controlflow of multiple Chainlets in more complicated Chains.

To do this, define a fake Mistral model class whose `run`-method implements the same *Protocol* as the original
 `MistralLLM` Chainlet, and produces some test output. Then "inject" an instance of `FakeMistralLLM` into your Chain
like so:

```python
if __name__ == "__main__":
    class FakeMistralLLM:
        def run(self, data: str) -> str:
            return "Fake mistral response"

    with chains.run_local():
        poem_generator = PoemGenerator(mistral_llm=FakeMistralLLM())
        result = poem_generator.run(words=["bird", "plane", "superman"])
        print(result)
```

And run your Python file:

```
$ python poems.py
```

## Remote Deployment

To deploy your Chain remotely to Baseten, run the following command:

```bash
truss chain deploy ./poems.py PoemGenerator
```


See [full reference](/chains/full-reference) for more details on the full Chains API.
