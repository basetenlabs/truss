---
title: Reference
description: "Truss Chains Reference"
---

<Warning>This is a beta feature and subject to breaking changes.</Warning>

# chains.ProcessorBase

This is the base class from which all Processors inherit. The main class-level attributes that one
can define here are:

* `remote_config` -- This is a `RemoteConfig` object that defines how the Processor should be deployed.
* `default_user_config` -- This is an optional pydantic.BaseModel that defines the default user configuration
for the Processor.

You then define a `run` method on the Processor that contains the logic for the actual processor:

```python
class HelloWorld(chains.ProcessorBase):
    remote_config = chains.RemoteConfig(
        docker_image=chains.DockerImage()
    )

    def run(self) -> str:
        return "Hello World!"
```

# chains.deploy_remotely

```python
chains.deploy_remotely(processor: Type[chains.ProcessorBase], workflow_name: str)
```

The `deploy_remotely` function takes a processor and a user-defined `workflow_name` and deploys the processor
to a remote (just Baseten for now).

# chains.provide

```python
chains.provide(processor: Type[chains.ProcessorBase])
```

`chains.provide` is a key part of the chains Dependency Injection framework, and is used for processors
to reference other processors in a chain.


```python
class MyProcessor(chains.ProcessorBase):
    def __init__(
        self,
        other_processor: OtherProcessor = chains.provide(MyProcessor)
    ):
        self._other_processor = other_processor

    def run(self):
        return self._other_processor.run()
```

# chains.provide_context

```python
chains.provide_context()
```

Returns a `DeploymentContext` object that contains information about the current deployment context, such
as secrets.

# chains.RemoteConfig

The `chains.RemoteConfig` object is used to specify the configuration for the _remote deployment of a processor_.
In other words, all of the options for how a processor should be deployed on a remote server (what docker image should be used,
what dependencies need to be installed, what compute is needed, etc.)

**Signature:**

```python
chains.RemoteConfig(
    docker_image=chains.DockerImage,
    compute=chains.Compute,
    assets=chains.Assets,
    name=Optional[str]
)
```

You can use this by assigning an instance to the `remote_config` attribute of a Processor:

```python
class MyProcessor(chains.BaseProcessor):
    remote_config = chains.RemoteConfig(
        docker_image=chains.DockerImage(),
        compute=...,
        assets=...,
        name="my-processor"
    )
    ...
```

# chains.Assets

# chains.Compute


# chains.DeploymentContext

This is an object that is provided to Processors via `chains.provide_context()`. It contains information
about the current deployment context.

## chains.DeploymentContext.secrets

Returns a dictionary of secrets that are available to the Processor.

```
context.secrets # {'my_secret': 'my_secret_value'}
```

# chains.DockerImage

A builder object used to define the Docker image that a Processor should run in.
Pass this builder object to the RemoteConfig object on the Processor like so:

```python
chains.RemoteConfig(docker_image=chains.DockerImage())
```

## chains.DockerImage.base_image

Specify the base image you'd like to use for the image build.

```python
chains.DockerImage().base_image("python:3.8") # Use Python 3.8 from dockerhub as the base image
```

## chains.DockerImage.pip_requirements_file

Specify a requirements file to install dependencies from.

```python
# Reference a file called "requirements.txt" in the current directory

chains.DockerImage().pip_requirements_file("requirements.txt")
```

## chains.DockerImage.pip_requirements
```python
# Reference a file called "requirements.txt" in the current directory

chains.DockerImage().pip_requirements_file("requirements.txt")
```
# chains.UsageError




# chains.run_local

# chains.make_abs_path_here


Content here
