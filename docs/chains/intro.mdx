---
title: Welcome to Truss Chains (Î²)
description: "Learn what *Truss Chains* is"
---

<Warning>This is a beta feature and subject to breaking changes.</Warning>

# Table of Contents

* [Introduction](/chains/intro)
* [Getting Started](/chains/getting-started)
* [Chaining Chainlets](/chains/chaining-chainlets)
* [Dependencies & Resources](/chains/deps-and-resources)
* [Reference](/chains/full-reference)

With a a vanilla Truss, you can call a single model in an HTTP Request. However, many inference workloads are
more complex -- you might want to call multiple models in a sequence, or partition your input data into
smaller chunks and call a model for each chunk. *Truss Chains* is a framework for implementing these complex workloads.

# Key Concepts

**Chainlet**: The basic building block in a Truss Chain: it performs a specific computation. For example, running a
large ML model, but it can also be a simple function that transforms data. The most interesting aspect
is that a *chainlet() calls other *chainlets* .  Each *chainlet* is intended to be deployed remotely, with
potentially multiple replicas and its own environment (e.g. compute hardware, autoscaling, dependencies).

<img src="/images/audio-transcription-chainlet.png" />

**Chain**: A *chain* is composed of multiple chainlets that are linked to each other. Each *chain* has an
"*entrypoint*" chainlet to which requests are made by the client. Internally, the different chainlets call each other,
thereby structuring and distributing the overall computation into smaller parts - and eventually the entrypoint
chainlet returns the end-to-end result back to the client.
You can imagine this like a "flow chart" or "computational graph".

Here's an example of a chain that takes a large audio file, splits it into smaller chunks, and transcribes each chunk
*in parallel* to speed up the transcription process.

<img src="/images/audio-transcription-chain.png" />

## Remote vs. Local

Keep in my mind that Truss Chains is designed to run in a distributed environment, with each chainlet running
with its own set of resources and autoscaling settings. To enable a smooth development experience, Truss Chains
also supports a *local development mode* where you can run your chainlets on your local machine.

To facilitate the distributed execution, chainlets must be referenced and sub-classed in a particular way--more on this soon.

# Hello World

Chains are powerful and have more advanced features explained in following sections. For starters, here is
a minimal "Hello World" example of Truss Chains.

```python
import truss_chains as chains

class HelloWorld(chains.ChainletBase):
    remote_config = chains.RemoteConfig(
        docker_image=chains.DockerImage()
    )

    def run(self, num_repetitions: int) -> str:
        # Your user code goes here.
        return "Hello World! " * num_repetitions
```

See [Getting Started](/chains/getting-started) for how to run this!
