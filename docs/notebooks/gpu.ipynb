{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Truss on GPU\n",
    "\n",
    "In this guide, we'll walk through how to use Truss on a GPU. We'll build a model that uses the GPU, turn it into a Truss and run the Truss as GPU-enabled on an AWS instance. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Getting a GPU server on AWS\n",
    "\n",
    "We'll start by setting up our GPU instance. \n",
    "1. Navigate to the [EC2](https://us-west-2.console.aws.amazon.com/ec2/) dashboard and select the __Create Instance__ button on the top right. \n",
    "2. You'll need to select an instance. [This](https://instances.vantage.sh/?region=us-west-2) page might be helpful if you want to know more about GPU instance pricing (just search for \"GPU\" in the *Name* column.). We'll be using a `p2.xlarge`. \n",
    "3. Next, we'll select an AMI. Specifically, we'll use the *Deep Learning AMI GPU PyTorch 1.12.0 (Amazon Linux 2)* AMI. This AMI is free and comes with a lot of neccesary setup for GPU support. When looking at GPU AMIs, we want an AMI that supports CUDA, CUDNN and the relevant library we're using (in this case PyTorch). \n",
    "4. Generate a key pair, this'll be important when we connect to our instance. Make sure to download the resulting `.cer` file. \n",
    "5. Press __Launch Instance__ on the bottom right. \n",
    "\n",
    "# Connecting to your GPU server \n",
    "\n",
    "Now that we have a GPU server, let's connect to it. \n",
    "1. On your AWS dashboard, identify the `Public IPv4 DNS`. This is the link we'll use to access our instance. \n",
    "2. In your local terminal, execute the following command. This'll SSH us into our GPU server. \n",
    "```\n",
    "ssh -i [PATH_TO_.CER_FILE] ec2-user@[IPV4_DNS_ADDRESS]\n",
    "```\n",
    "3. If successful, you should now see your terminal user as `ec2-user@ip-[IPV4_ADDRESS]`. You should also be able to run `nvidia-smi` to see details about the GPU and NVidia drivers on your system. \n",
    "\n",
    "# Setting up GPU server\n",
    "We'll need to install a couple packages before we can create and run our model. \n",
    "1.  We'll want to activate the PyTorch enviroment that comes with our AMI \n",
    "``` \n",
    "source activate pytorch \n",
    "```\n",
    "\n",
    "2. We'll also want to install Truss and the Transformers package \n",
    "``` \n",
    "pip install --upgrade truss transformers\n",
    "```\n",
    "3. Let's create a directory where our model will live\n",
    "```\n",
    "mkdir dialo\n",
    "cd dialo\n",
    "```\n",
    "4. We'll also want to make sure that `docker buildx` is installed. If running `docker buildx` results in a command not found, we can install it via \n",
    "```\n",
    "LATEST=$(wget -qO- \"https://api.github.com/repos/docker/buildx/releases/latest\" | jq -r .name)\n",
    "wget https://github.com/docker/buildx/releases/download/$LATEST/buildx-$LATEST.linux-amd64\n",
    "chmod a+x buildx-$LATEST.linux-amd64\n",
    "mkdir -p ~/.docker/cli-plugins\n",
    "mv buildx-$LATEST.linux-amd64 ~/.docker/cli-plugins/docker-buildx\n",
    "```\n",
    "\n",
    "5. Finally, we'll initialize a base Truss scaffold \n",
    "```\n",
    "truss init ./ \n",
    "``` \n",
    "\n",
    "# Creating our model \n",
    "We'll be using the Microsoft `DialoGPT` model, designed to be able to converse with users. Let's replace the base `model.py` with our `model.py` file below to be GPU compatible (read the comments in code snippet)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Our model.py file\n",
    "from typing import Dict, List\n",
    "\n",
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "class Model(object):\n",
    "    def __init__(self, **kwargs) -> None:\n",
    "        self._data_dir = kwargs['data_dir']\n",
    "        self._config = kwargs['config']\n",
    "        self._model = None\n",
    "\n",
    "    def load(self):\n",
    "        # First thing, in our load function, we'll want to set an attribute `device`. This will help us push \n",
    "        # certain tensors / inputs to our GPU instead of the CPU. This is useful because when we run inference \n",
    "        # on our GPU, we want the data we're using to live on the GPU for faster inference times. \n",
    "        self.device = 0 if torch.cuda.is_available() else 'cpu'\n",
    "        self._tokenizer = AutoTokenizer.from_pretrained(\"microsoft/DialoGPT-large\")\n",
    "        # Now, we'll load our model onto the GPU. In order to utilize the GPU, both the model and any \n",
    "        # variable needed for inference (like the tokenized input) need to be `.to(self.device)`. \n",
    "        self._model = AutoModelForCausalLM.from_pretrained(\"microsoft/DialoGPT-large\").to(self.device)\n",
    "        self.ready = True\n",
    "\n",
    "    def preprocess(self, request: Dict) -> Dict:\n",
    "        \"\"\"\n",
    "        Incorporate pre-processing required by the model if desired here.\n",
    "\n",
    "        These might be feature transformations that are tightly coupled to the model.\n",
    "        \"\"\"\n",
    "        return request\n",
    "\n",
    "    def postprocess(self, request: Dict) -> Dict:\n",
    "        \"\"\"\n",
    "        Incorporate post-processing required by the model if desired here.\n",
    "        \"\"\"\n",
    "        return request\n",
    "\n",
    "    def predict(self, request: Dict) -> Dict[str, List]:\n",
    "        response = {}\n",
    "        inputs = request['inputs'] # noqa\n",
    "        with torch.no_grad():\n",
    "            conversation_history = []\n",
    "            for message in inputs:\n",
    "                text = message['prompt'] + self._tokenizer.eos_token\n",
    "                text_ids = self._tokenizer.encode(text, return_tensors='pt')\n",
    "                conversation_history.append(text_ids)\n",
    "            # Like mentioned above, this is where we \"push\" our input to the GPU. \n",
    "            # The '.to(self.device)' pushes the Torch Tensor to our GPU. \n",
    "            conversation_history_torch = torch.cat(conversation_history, dim=-1).to(self.device)\n",
    "            chat_history_ids = self._model.generate(conversation_history_torch, max_length=500, pad_token_id=self._tokenizer.eos_token_id)\n",
    "            # NOTE: You cannot map the tokenizer to the GPU. Just the model and any input to the model. \n",
    "            decoded_response = self._tokenizer.decode(chat_history_ids[:, conversation_history_torch.shape[-1]:][0], skip_special_tokens=True)\n",
    "        response['response'] = decoded_response\n",
    "        return response"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll also need to edit our `config.yaml` file. Specifically, underneath the `resources` key, we'll remove all the keys and keep the `GPU` key. We'll set that to `True` to tell Truss to run our model in GPU mode. Your `config.yaml` should look something like this \n",
    "\n",
    "```\n",
    "data_dir: data\n",
    "environment_variables: {}\n",
    "examples_filename: examples.yaml\n",
    "input_type: Any\n",
    "model_class_filename: model.py\n",
    "model_class_name: Model\n",
    "model_framework: custom\n",
    "model_metadata: {}\n",
    "model_module_dir: model\n",
    "model_name: null\n",
    "model_type: custom\n",
    "python_version: py39\n",
    "requirements: []\n",
    "resources:\n",
    "  use_gpu: true\n",
    "secrets: {}\n",
    "system_packages: []\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Running our GPU-enabled Truss \n",
    "We've setup our GPU instance, defined our model, made it GPU-compatible, and updated our `config` to let Truss know to use the GPU. Let's run our model now! \n",
    "\n",
    "We can run our model by using the Truss CLI. To begin, we'll run our model locally with the `--run-local` flag. \n",
    "\n",
    "```\n",
    "truss predict --target_directory ./ --request '{\"inputs\" : [{\"prompt\" : \"Hi whats your name\"}]}' --run-local\n",
    "```\n",
    "The model should respond to your question. To continue the conversation, we just pass another prompt object in the request. \n",
    "\n",
    "```\n",
    "truss predict --target_directory ./ --request '{\"inputs\" : [{\"prompt\" : \"Hi whats your name\"}, {\"prompt\" : \"Im not sure what you mean by that\"}]}' --run-local\n",
    "```\n",
    "\n",
    "We can also run the model inside a Docker image. This is useful when you want to deploy your Truss on a server all packaged up. \n",
    "```\n",
    "truss predict --target_directory ./ --request '{\"inputs\" : [{\"prompt\" : \"Hi whats your name\"}]}'\n",
    "```"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.2 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.9.2"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
