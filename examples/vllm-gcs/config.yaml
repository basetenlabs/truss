build:
  arguments:
    endpoint: Completions
    model: /app/hf_cache/llama-2-7b
  model_server: VLLM
environment_variables: {}
external_package_dirs: []
model_metadata: {}
model_name: null
python_version: py39
requirements:
- huggingface_hub
- google-cloud-storage
resources:
  accelerator: A10G
  cpu: 500m
  memory: 30Gi
  use_gpu: True
hf_cache:
- repo_id: gs://llama-2-7b
secrets: {}
system_packages: []
