build:
  arguments:
    endpoint: Completions
    model: /app/hf_cache/llama-2-7b
    tokenizer: hf-internal-testing/llama-tokenizer
  model_server: VLLM
environment_variables: {}
external_package_dirs: []
hf_cache:
- repo_id: gs://llama-2-7b
model_metadata: {}
model_name: vllm llama gcs
python_version: py39
requirements:
- google-cloud-storage
resources:
  accelerator: A10G
  cpu: 500m
  memory: 30Gi
  use_gpu: true
secrets: {}
system_packages: []
